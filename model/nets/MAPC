import torch
from torch import nn, einsum
from thop import profile


class PConv(nn.Module):
    def __init__(self, dim, n_div=4):
        super().__init__()
        self.dim_conv3 = dim // n_div
        self.dim_untouched = dim - self.dim_conv3
        self.partial_conv3 = nn.Conv2d(self.dim_conv3, self.dim_conv3, 3, 1, 1, bias=False)

    def forward(self, x):
        # for training/inference
        x1, x2 = torch.split(x, [self.dim_conv3, self.dim_untouched], dim=1)
        x1 = self.partial_conv3(x1)
        x = torch.cat((x1, x2), 1)
        return x


class MAPC(nn.Module):
    def __init__(self, in_channels_pre, in_channels_post, dim):
        super().__init__()

        self.downsample_pre =nn.Sequential(
            nn.Conv2d(in_channels_pre, dim, kernel_size=3, padding=1, bias=False, stride=2),
            nn.BatchNorm2d(dim),
            nn.ReLU(inplace=True),
        ).cuda()

        self.downsample_post = nn.Sequential(
            nn.Conv2d(in_channels_post, dim, kernel_size=3, padding=1, bias=False, stride=2),
            nn.BatchNorm2d(dim),
            nn.ReLU(inplace=True),
        ).cuda()

        self.PConv = PConv(dim).cuda()

        self.AConv1 = nn.Sequential(
            nn.Conv2d(dim, dim, kernel_size=(1, 3), padding=(0, 1), groups=dim),
            nn.Conv2d(dim, dim, kernel_size=(3, 1), padding=(1, 0),  groups=dim),
            nn.BatchNorm2d(dim),
            nn.ReLU(inplace=True),
        ).cuda()

        self.AConv2 = nn.Sequential(
            nn.Conv2d(dim, dim, kernel_size=(1, 5), padding=(0, 2), groups=dim),
            nn.Conv2d(dim, dim, kernel_size=(5, 1), padding=(2, 0), groups=dim),
            nn.BatchNorm2d(dim),
            nn.BatchNorm2d(dim),
            nn.ReLU(inplace=True),
        ).cuda()

        self.AConv3 = nn.Sequential(
            nn.Conv2d(dim, dim, kernel_size=(1, 7), padding=(0, 3), groups=dim ),
            nn.Conv2d(dim, dim, kernel_size=(7, 1), padding=(3, 0),  groups=dim),
            nn.BatchNorm2d(dim),
            nn.ReLU(inplace=True),
        ).cuda()

        self.conv = nn.Sequential(
            nn.Conv2d(dim*3, dim, kernel_size=1),
            nn.BatchNorm2d(dim),
            nn.ReLU(inplace=True),
        ).cuda()

    def forward(self, pre, post):
        pre = pre.cuda()
        post = post.cuda()
        pre0 = self.downsample_pre(pre)
        post0 = self.downsample_post(post)

        pre1 = self.PConv(pre0)
        pre1 = self.AConv1(pre1)
        pre2 = self.PConv(pre0)
        pre2 = self.AConv2(pre2)
        pre3 = self.PConv(pre0)
        pre3 = self.AConv3(pre3)
        pre = torch.cat((pre1, pre2, pre3), dim=1)
        pre = self.conv(pre)
        pre = pre + pre0

        post1 = self.PConv(post0)
        post1 = self.AConv1(post1)
        post2 = self.PConv(post0)
        post2 = self.AConv2(post2)
        post3 = self.PConv(post0)
        post3 = self.AConv3(post3)
        post = torch.cat((post1, post2, post3), dim=1)
        post = self.conv(post)
        post = post + post0

        return pre, post


if __name__ == "__main__":
    model = MAPC(in_channels_pre=3, in_channels_post=1, dim=64)
    model.eval()
    pre = torch.randn(1, 3, 256, 256)
    post = torch.randn(1, 1, 256, 256)
    with torch.no_grad():
        pre0, post0 = model.forward(pre, post)
    print(pre0.shape, post0.shape)
    model.cuda()
    flops, params = profile(model, (pre.cuda(), post.cuda()))
    print('flops: %.2f GFLOPS, params: %.2f M' % (flops / 1000000000.0, params / 1000000.0))
